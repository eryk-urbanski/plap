{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(          label  class       ASC   ASC_VAR       ASS   ASS_VAR      ASF1  \\\n",
       " 0   EB0_anger_1  anger -1.014424  1.664194  1.316892  0.381699  0.018763   \n",
       " 1  EB0_anger_10  anger -0.981852  0.980242  1.067413  0.267500  0.028557   \n",
       " 2  EB0_anger_11  anger -1.308893  0.833898  1.005187  0.196945  0.028928   \n",
       " 3  EB0_anger_12  anger -1.196214  1.190325  1.099372  0.384295  0.036835   \n",
       " 4  EB0_anger_13  anger -1.061688  1.266889  1.203192  0.414269  0.039796   \n",
       " \n",
       "        ASF2      ASF3      ASF4  ...     MFCC11    MFCC12    MFCC13  \\\n",
       " 0  0.019457  0.021270  0.022729  ...  -6.794261 -1.364080 -8.251892   \n",
       " 1  0.029120  0.035248  0.035732  ...  -7.334883  0.527785 -9.577043   \n",
       " 2  0.040296  0.045253  0.046051  ... -12.745518  4.687151 -2.866292   \n",
       " 3  0.040647  0.046487  0.049039  ... -10.367361  0.666284 -9.921471   \n",
       " 4  0.043942  0.046717  0.047542  ... -16.382733  1.155685 -5.097772   \n",
       " \n",
       "       MFCC14    MFCC15     MFCC16     MFCC17    MFCC18    MFCC19     MFCC20  \n",
       " 0  -7.603094 -1.047732 -11.414605  -8.424419 -1.904299 -5.137508  -7.206445  \n",
       " 1 -12.632370 -2.637967  -7.211876  -9.454415 -9.712543 -7.476727  -9.609391  \n",
       " 2 -15.424924 -0.789306  -8.345286 -10.642456 -6.398707 -5.329722 -10.836348  \n",
       " 3 -13.131100  2.942820  -8.700433  -8.802478 -8.299896 -9.789159  -9.863605  \n",
       " 4 -10.731965  2.700978 -10.230451  -8.155475 -7.084869 -7.409772  -7.801661  \n",
       " \n",
       " [5 rows x 129 columns],\n",
       " 4481)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def csv_values_to_numpy(file_path) -> np.ndarray:\n",
    "    # Read the file and split values by commas\n",
    "    array = np.loadtxt(file_path, delimiter=',')\n",
    "    # Convert the data into a numpy array of floats\n",
    "    numpy_array = np.array(array, dtype=float)\n",
    "    return numpy_array\n",
    "\n",
    "# Emotions in the RAVDESS dataset\n",
    "emotions = {\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "# all_timbral_spectral_python = all_fvector\n",
    "# dataset = pd.read_csv(\"./python_csv/ravdess_3classes_preemphasis.csv\")\n",
    "# dataset = pd.read_csv(\"./python_csv/ravdess_wholedataset.csv\")\n",
    "dataset = pd.read_csv(\"./python_csv/nEMO_wholedataset.csv\")\n",
    "dataset = dataset.fillna(0)\n",
    "# dataset = pd.read_csv(\"./python_csv/ravdess_newest.csv\")\n",
    "# dataset = pd.read_csv(\"./python_csv/ravdess_newfvector.csv\")\n",
    "# dataset = pd.read_csv(\"./python_csv/ravdess_bs2048hamming_ov50_nopreemph.csv\")\n",
    "dataset = dataset.drop_duplicates(subset='label')\n",
    "dataset.head(5), len(dataset)\n",
    "# dataset.iloc[40:4000:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector statistics:    min = -1070.190,    max = 5704079.508,    mean = 16373.052,    deviation = 190434.915\n"
     ]
    }
   ],
   "source": [
    "# We would usually use df.describe(), but it provides a bit of a mess of information we don't need at the moment.\n",
    "def print_features(df):\n",
    "    # Check MFCC feature values\n",
    "    features_df = df.iloc[:,2:]\n",
    "    # print(features_df.shape)\n",
    "    # print(features_df.head(5))\n",
    "    feature_min = features_df.min().min()\n",
    "    feature_max = features_df.max().max()\n",
    "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
    "    feature_mean = features_df.stack().mean()\n",
    "    feature_stdev = features_df.stack().std()\n",
    "    print(f'Feature vector statistics:\\\n",
    "    min = {feature_min:.3f},\\\n",
    "    max = {feature_max:.3f},\\\n",
    "    mean = {feature_mean:.3f},\\\n",
    "    deviation = {feature_stdev:.3f}')\n",
    "print_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM tuning using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def perform_cross_val(X, y, classifier):\n",
    "    scaler = StandardScaler() # Gaussian with zero mean and unit variance.\n",
    "    features_scaled = X\n",
    "    features_scaled = scaler.fit_transform(features_scaled)\n",
    "    print('Standard Scaling:')\n",
    "    features_scaled_df = pd.DataFrame(features_scaled)\n",
    "    print_features(features_scaled_df)\n",
    "    scores = cross_val_score(classifier, features_scaled, y, cv=5)\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM and k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_models(X, y, scaler = None, perform_pca: bool = False, pca_components: int = 2, C = 10, gamma = 0.125):\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    # keep our unscaled features just in case we need to process them alternatively\n",
    "    features_scaled = X\n",
    "    features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "    if perform_pca:\n",
    "        pca = PCA(n_components=pca_components)\n",
    "        features_scaled = pca.fit_transform(features_scaled)\n",
    "        # print(f\"explained ratio: {pca.explained_variance_ratio_}\")\n",
    "        # print(f\"explained ratio sum: {sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "    # print('Standard Scaling:')\n",
    "    # features_scaled_df = pd.DataFrame(features_scaled)\n",
    "    # print_features(features_scaled_df)\n",
    "\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "        features_scaled,\n",
    "        y,\n",
    "        test_size=0.2, \n",
    "        random_state=69\n",
    "    )\n",
    "\n",
    "    classification_models = [\n",
    "        KNeighborsClassifier(\n",
    "            n_neighbors = 3,\n",
    "            weights = 'distance',\n",
    "            algorithm = 'brute',\n",
    "            leaf_size = 30,\n",
    "            n_jobs=4\n",
    "        ),\n",
    "        SVC(kernel='linear'),\n",
    "        SVC(\n",
    "            C=C,\n",
    "            gamma=gamma,\n",
    "            kernel='rbf',\n",
    "            random_state=2137\n",
    "        ),\n",
    "        ]\n",
    "\n",
    "    scores = []\n",
    "    # Create subplots for 1x3 layout\n",
    "    # fig, axes = plt.subplots(3, 1, figsize=(6, 18))  # Adjust the figure size as needed\n",
    "\n",
    "    for i, model in enumerate(classification_models):\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        score_train = model.score(X_train_scaled, y_train)\n",
    "        score = model.score(X_test_scaled, y_test)\n",
    "        model_name = type(model).__name__\n",
    "        if model_name == 'SVC' and model.kernel == 'rbf': \n",
    "            model_name += ' RBF kernel'\n",
    "        scores.append((model_name, f'{100 * score_train:.2f}%', f'{100 * score:.2f}%'))\n",
    "\n",
    "        # # Confusion matrix\n",
    "        # predictions = model.predict(X_test_scaled)\n",
    "        # cm = confusion_matrix(y_test, predictions, labels=model.classes_)\n",
    "        # labels_pl_nEMO = [\"złość\", \"strach\", \"szczęście\", \"neutralność\", \"smutek\", \"zaskoczenie\"]\n",
    "        \n",
    "        # # Use subplot axes\n",
    "        # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_pl_nEMO)\n",
    "        # disp.plot(ax=axes[i], colorbar=False)  # Add `colorbar=False` to simplify the layout\n",
    "        # model_names_pl = [\"k-NN\", \"SVM, jądro libiowe\", \"SVM, jądro RBF\"]\n",
    "        # axes[i].set_title(f\"Macierz pomyłek: {model_names_pl[i]}\")\n",
    "        # axes[i].set_xticklabels(labels_pl_nEMO, rotation=45, ha='right')  # Rotate x-ticks\n",
    "        # axes[i].set_xlabel(\"Klasa przewidywana\")\n",
    "        # axes[i].set_ylabel(\"Klasa rzeczywista\")\n",
    "\n",
    "    # Make it pretty\n",
    "    scores_df = pd.DataFrame(scores,columns=['Classifier','Train Accuracy Score', 'Test Accuracy Score'])\n",
    "    \n",
    "    print(scores_df.sort_values(by='Test Accuracy Score',axis=0,ascending=False))\n",
    "    # Adjust layout to avoid overlap\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_pca = True\n",
    "pca_components = 20\n",
    "# Tuning SVM RBF\n",
    "C = 2**3\n",
    "gamma = 2**-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subset_map = {\n",
    "    'BasicSpectral': range(2,56),\n",
    "    'SignalParameters': range(56, 58),\n",
    "    'TimbralTemporal': range(58,60),\n",
    "    'TimbralSpectral': range(60,66),\n",
    "    'SpectralBasis': range(66,109),\n",
    "    'MFCC': range(109,129),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Classifier Train Accuracy Score Test Accuracy Score\n",
      "2        SVC RBF kernel               95.84%              66.89%\n",
      "0  KNeighborsClassifier              100.00%              57.97%\n",
      "1                   SVC               50.70%              49.72%\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: All Available MPEG-7\n",
    "columns_to_remove = list(feature_subset_map['MFCC'])\n",
    "dataset_1 = dataset.drop(dataset.columns[columns_to_remove], axis=1)\n",
    "X_1, y_1 = dataset_1.iloc[:,2:], dataset_1.iloc[:,1]\n",
    "train_models(X_1, y_1, perform_pca=perform_pca, pca_components=pca_components, C=C, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Classifier Train Accuracy Score Test Accuracy Score\n",
      "2        SVC RBF kernel               94.34%              76.25%\n",
      "0  KNeighborsClassifier              100.00%              67.67%\n",
      "1                   SVC               59.77%              58.97%\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Basic Spectral + Timbral\n",
    "columns_to_remain = [0,1] + list(feature_subset_map['BasicSpectral']) + list(feature_subset_map['TimbralTemporal']) + list(feature_subset_map['TimbralSpectral'])\n",
    "dataset_2 = dataset.iloc[:, columns_to_remain]\n",
    "X_2, y_2 = dataset_2.iloc[:,2:], dataset_2.iloc[:,1]\n",
    "train_models(X_2, y_2, perform_pca=perform_pca, pca_components=pca_components, C=C, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Classifier Train Accuracy Score Test Accuracy Score\n",
      "2        SVC RBF kernel               96.65%              88.52%\n",
      "0  KNeighborsClassifier              100.00%              84.50%\n",
      "1                   SVC               61.22%              56.63%\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: Only MFCCs\n",
    "columns_to_remain = [0,1] + list(feature_subset_map['MFCC'])\n",
    "dataset_3 = dataset.iloc[:, columns_to_remain]\n",
    "X_3, y_3 = dataset_3.iloc[:,2:], dataset_3.iloc[:,1]\n",
    "train_models(X_3, y_3, perform_pca=perform_pca, pca_components=pca_components, C=C, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Classifier Train Accuracy Score Test Accuracy Score\n",
      "2        SVC RBF kernel               99.89%              88.85%\n",
      "0  KNeighborsClassifier              100.00%              82.16%\n",
      "1                   SVC               62.28%              59.98%\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4: Basic Spectral + Timbral + 20 MFCCs\n",
    "columns_to_remain = [0,1] + list(feature_subset_map['BasicSpectral']) + list(feature_subset_map['TimbralTemporal']) + list(feature_subset_map['TimbralSpectral']) + list(feature_subset_map['MFCC'])\n",
    "dataset_4 = dataset.iloc[:, columns_to_remain]\n",
    "X_4, y_4 = dataset_4.iloc[:,2:], dataset_4.iloc[:,1]\n",
    "train_models(X_4, y_4, perform_pca=perform_pca, pca_components=pca_components, C=C, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Classifier Train Accuracy Score Test Accuracy Score\n",
      "2        SVC RBF kernel               87.30%              75.25%\n",
      "0  KNeighborsClassifier              100.00%              66.44%\n",
      "1                   SVC               56.19%              57.08%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=20 must be between 0 and min(n_samples, n_features)=2 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m dataset_5 \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39miloc[:, columns_to_remain_instance]\n\u001b[0;32m     11\u001b[0m X_5, y_5 \u001b[38;5;241m=\u001b[39m dataset_5\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:], dataset_5\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperform_pca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperform_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[144], line 19\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(X, y, scaler, perform_pca, pca_components, C, gamma)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m perform_pca:\n\u001b[0;32m     18\u001b[0m     pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mpca_components)\n\u001b[1;32m---> 19\u001b[0m     features_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# print(f\"explained ratio: {pca.explained_variance_ratio_}\")\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# print(f\"explained ratio sum: {sum(pca.explained_variance_ratio_)}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print('Standard Scaling:')\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# features_scaled_df = pd.DataFrame(features_scaled)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print_features(features_scaled_df)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m X_train_scaled, X_test_scaled, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     28\u001b[0m     features_scaled,\n\u001b[0;32m     29\u001b[0m     y,\n\u001b[0;32m     30\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[0;32m     31\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m69\u001b[39m\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_pca.py:514\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_pca.py:530\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    527\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m         )\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_components, \u001b[38;5;28mmin\u001b[39m(n_samples, n_features))\n\u001b[0;32m    534\u001b[0m     )\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# Center data\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=20 must be between 0 and min(n_samples, n_features)=2 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "# Experiment 5: Subcategories of MPEG-7\n",
    "columns_to_remain_list = [\n",
    "    [0,1] + list(feature_subset_map['BasicSpectral']),\n",
    "    [0,1] + list(feature_subset_map['SignalParameters']),\n",
    "    [0,1] + list(feature_subset_map['TimbralTemporal']),\n",
    "    [0,1] + list(feature_subset_map['TimbralSpectral']),\n",
    "    [0,1] + list(feature_subset_map['SpectralBasis']),\n",
    "]\n",
    "for columns_to_remain_instance in columns_to_remain_list:\n",
    "    dataset_5 = dataset.iloc[:, columns_to_remain_instance]\n",
    "    X_5, y_5 = dataset_5.iloc[:,2:], dataset_5.iloc[:,1]\n",
    "    train_models(X_5, y_5, perform_pca=perform_pca, pca_components=pca_components, C=C, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Tune Experiment2\n",
    "# # X_tune, y_tune = X_2, y_2\n",
    "# # Tune Experiment3\n",
    "# # X_tune, y_tune = X_3, y_3\n",
    "# # Tune Experiment4\n",
    "# X_tune, y_tune = X_4, y_4\n",
    "# # X, y = dataset.iloc[:,2:], dataset.iloc[:,1]\n",
    "# # X.shape, y.shape\n",
    "\n",
    "# exponents_c = list(range(-5,15,2))\n",
    "# exponents_gamma = list(range(-15,3,2))\n",
    "\n",
    "# for exponent_c in exponents_c:\n",
    "#     for exponent_gamma in exponents_gamma:\n",
    "#         print(f\"exponent_c={exponent_c} exponent_gamma={exponent_gamma}\")\n",
    "#         classifier = SVC(\n",
    "#             C=2**exponent_c,\n",
    "#             gamma=2**exponent_gamma,\n",
    "#             kernel='rbf',\n",
    "#             random_state=2137,\n",
    "#         )\n",
    "#         perform_cross_val(X=X_tune,y=y_tune,classifier=classifier)\n",
    "#         print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Define the SVM model\n",
    "# svm = SVC(kernel='rbf', random_state=2137)\n",
    "\n",
    "# # Define the parameter grid for C and gamma\n",
    "# exponents_c = list(range(-5,15,2))\n",
    "# exponents_gamma = list(range(-15,3,2))\n",
    "# param_grid = {\n",
    "#     'C': [2 ** exp for exp in exponents_c],\n",
    "#     'gamma': [2 ** exp for exp in exponents_gamma]\n",
    "# }\n",
    "\n",
    "# # Perform grid search with cross-validation\n",
    "# grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "# # Tune Experiment2\n",
    "# X_tune, y_tune = X_2, y_2\n",
    "# # Tune Experiment3\n",
    "# # X_tune, y_tune = X_3, y_3\n",
    "# # Tune Experiment4\n",
    "# # X_tune, y_tune = X_4, y_4\n",
    "# grid_search.fit(X_tune, y_tune)\n",
    "\n",
    "# # Best parameters\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Best score\n",
    "# print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 0.03125)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "# import plap\n",
    "# import numpy as np\n",
    "\n",
    "# block_size = 1024\n",
    "# overlap = 50\n",
    "# step = int((100 - overlap) / 100 * block_size)\n",
    "# window_type = \"hamming\"\n",
    "\n",
    "# input_file = \"C:/Users/windo/Documents/ravdess-dataset/Actor_21/03-01-01-01-01-02-21.wav\"\n",
    "# preprocessor = plap.Preprocessor(preemphasis_coeff=0.68, block_size=block_size, window_type=window_type)\n",
    "# fvector = plap.FeatureVector(\"asc\")\n",
    "# # fvector = plap.FeatureVector(\"ase\",\"asf\")\n",
    "\n",
    "# plap.parameterize(audio_path=input_file, fvector=fvector, preprocessor=preprocessor)\n",
    "\n",
    "# import librosa\n",
    "# signal, sr = librosa.load(input_file, sr=None)\n",
    "\n",
    "# power_spectrum = np.abs(librosa.stft(y=signal, n_fft=block_size, hop_length=step, window=window_type)) ** 2\n",
    "\n",
    "\n",
    "# # Find ASF middle of band freqs\n",
    "# low_edge = 250\n",
    "# high_edge = 16000\n",
    "# num_bands = int(np.floor(4 * np.log2(high_edge / low_edge)))\n",
    "# freqs = librosa.fft_frequencies(sr=sr, n_fft=block_size)\n",
    "\n",
    "# band_centers = []\n",
    "# for k in range(num_bands):\n",
    "#     # Get the frequency indices for the current band\n",
    "#     f_low = low_edge * (2 ** (k / 4))\n",
    "#     f_high = high_edge * (2 ** ((k+1) / 4))\n",
    "#     band_centers.append(np.searchsorted(freqs, (f_high+f_low)/2))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # times = librosa.times_like(power_spectrum)\n",
    "# # fig, ax = plt.subplots()\n",
    "# # librosa.display.specshow(librosa.amplitude_to_db(power_spectrum, ref=np.max),\n",
    "# #                          y_axis='log', x_axis='time', ax=ax)\n",
    "# # # ax.legend(loc='upper right')\n",
    "# # ax.set(title='log Power spectrogram')\n",
    "\n",
    "# # ase = fvector.values[:372]\n",
    "# # asf = fvector.values[373:]\n",
    "# # Plot\n",
    "# # plt.figure(figsize=(10, 5))\n",
    "# # plt.plot(ase, label='Array 1 (1, 372)', marker='o')\n",
    "# # plt.plot(band_centers, asf, label='Array 2 (1, 24)', marker='x')\n",
    "# # plt.legend()\n",
    "# # plt.title(\"Two Arrays with Different Lengths\")\n",
    "# # plt.xlabel(\"Index\")\n",
    "# # plt.ylabel(\"Value\")\n",
    "# # plt.show()\n",
    "# fvector.values.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
